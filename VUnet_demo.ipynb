{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Check if we get a GPU"
      ],
      "metadata": {
        "id": "8kciBXOxxgPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2XMSntmxcw6"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google colab now has fastai2, while the funcationaliry are essentially identical, they changed some library names between v1 and v2"
      ],
      "metadata": {
        "id": "8OnYWO9ixmBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==1.0.61"
      ],
      "metadata": {
        "id": "sz14sGztxf6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNyNysaA1oxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fastai.callback"
      ],
      "metadata": {
        "id": "ZPbJyaphxtAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!unzip -q sample_data/singletrialfish_Aug2022.zip\n",
        "fishdf=pd.read_csv('sample_data/trialfish_Aug2022_tmp.csv', header=0, index_col=0)"
      ],
      "metadata": {
        "id": "fzdaE7LDxvlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdf2 = []\n",
        "\n",
        "for i in range(400):\n",
        "  si = (i+1)*200-200\n",
        "  ei = (i+1)*200-100\n",
        "  tmpdf = fish[si:ei]\n",
        "  testdf2.append(tmpdf)\n",
        "newdf = pd.concat(testdf2)"
      ],
      "metadata": {
        "id": "pOcLZ8Wex24o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader for our particular dataset (#neuron X time X time => future movie frame)"
      ],
      "metadata": {
        "id": "JZUCXDB9yJ9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "import PIL\n",
        "\n",
        "def open_grammian_to_singleImage(fname):\n",
        "  mat=torch.load(fname)\n",
        "  mat = mat.type(torch.FloatTensor)\n",
        "  return Image(mat)\n",
        "\n",
        "class grammiansingleImageImageImageList(ImageImageList):\n",
        "    def open(self, fn):\n",
        "        return open_grammian_to_singleImage(fn)\n",
        "\n",
        "\n",
        "class Multi_to_MultiGrammianList(grammiansingleImageImageImageList):\n",
        "    \"`ItemList` suitable for `Image` to `Image` tasks.\"\n",
        "    _label_cls,_square_show,_square_show_res = grammiansingleImageImageImageList,False,False"
      ],
      "metadata": {
        "id": "qK6mEw04yD4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_cols=list(['trn1'])\n",
        "y_cols=list(['tst'])\n",
        "il = grammiansingleImageImageImageList.from_df(path='.',df=newdf_fish, cols=x_cols)\n",
        "ils = il.split_by_rand_pct(0.1, seed=42)\n",
        "#cls=list(myDict['tst'])\n",
        "tfms = get_transforms(flip_vert=False, do_flip=False, \n",
        "                      max_rotate=10, max_zoom=1.01, max_lighting=None, max_warp=None, \n",
        "                      p_affine=0., p_lighting=0.)\n",
        "ils2 = ils.label_from_df(cols=y_cols).transform(tfms, size=128, tfm_y=True)\n",
        "print(ils2)"
      ],
      "metadata": {
        "id": "y7ibr6riyBLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A usual GPU avail in Google colab has 16GB vram. Since we are using a pretrained VGG19 as our loss, we can at most fit in 32 images in one batch (500MB X 32) "
      ],
      "metadata": {
        "id": "RCZBNUGNyXHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs=32\n",
        "data = ils.databunch(bs=bs,num_workers=0)"
      ],
      "metadata": {
        "id": "cHVUTAg9yP36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the loss function"
      ],
      "metadata": {
        "id": "SmpsoQy3ymYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(x):\n",
        "    n,c,h,w = x.size()\n",
        "    x = x.view(n, c, -1)\n",
        "    return (x @ x.transpose(1,2))/(c*h*w)\n",
        "from torchvision.models import vgg19_bn\n",
        "from fastai.callbacks import *\n",
        "from fastai.utils.mem import *\n",
        "base_loss = F.l1_loss\n",
        "vgg_m = vgg19_bn(True).features.cuda().eval()\n",
        "requires_grad(vgg_m, False)\n",
        "blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)] #layer id \n",
        "blocks, [vgg_m[i] for i in blocks]\n",
        "print(blocks)\n",
        "#this feature loss works in case we use another pretrained network to compute the difference \n",
        "#if we do not do that, we will need to use a different loss function (VGG is way too memory expensive)\n",
        "class FeatureLoss(nn.Module):\n",
        "    def __init__(self, m_feat, layer_ids, layer_wgts):\n",
        "        super().__init__()\n",
        "        self.m_feat = m_feat #VGG19 is m_feat here\n",
        "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
        "        #Siwei why hooks here: keep the activations\n",
        "        self.hooks = hook_outputs(self.loss_features, detach=False) #need remove (no context manager here)\n",
        "        self.wgts = layer_wgts\n",
        "        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n",
        "              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n",
        "\n",
        "    def make_features(self, x, clone=False):\n",
        "        self.m_feat(x) #Siwei just copy (cloning it wonâ€™t involve autograd.)\n",
        "        \n",
        "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        out_feat = self.make_features(target, clone=True) #activations of VGG19 (clone the activations only)\n",
        "        in_feat = self.make_features(input) #activations of the Unet(resnet34)\n",
        "        self.feat_losses = [base_loss(input,target)] #w/ L1, this is l1 loss between input and target\n",
        "        self.feat_losses += [base_loss(f_in, f_out)*w #base_loss is L1\n",
        "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
        "        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3 #L1 between gram matrices\n",
        "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
        "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
        "        return sum(self.feat_losses)\n",
        "    \n",
        "    def __del__(self): self.hooks.remove()\n",
        "\n",
        "feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])"
      ],
      "metadata": {
        "id": "Ix31yEkqyg3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a U-net"
      ],
      "metadata": {
        "id": "GKrpYevpy20l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from VUnet_def.py import *"
      ],
      "metadata": {
        "id": "jR80ife61tWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wd = 1e-3\n",
        "learn = unet_vae_learner(data, models.resnet18, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,blur=True, norm_type=NormType.Weight, latentdims=(10,10), last_cross= False).to_fp16()\n",
        "gc.collect();"
      ],
      "metadata": {
        "id": "WBFAEneYzJo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "metadata": {
        "id": "tw-zJrOCzLcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkDWY3OkzREt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}